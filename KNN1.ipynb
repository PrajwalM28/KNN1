{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4369d1cc-7f49-423b-af9b-01e25454791b",
   "metadata": {},
   "source": [
    "# Q1.\n",
    "### What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82efe8c-270c-4c62-9300-1e9802026029",
   "metadata": {},
   "source": [
    "- K-Nearest Neighbors (KNN) is a simple and effective algorithm used for both classification and regression tasks in machine learning. It works based on the assumption that similar things exist in close proximity. Given a new data point, KNN algorithm identifies the 'K' nearest neighbors based on a distance measure and assigns the majority class (for classification) or average value (for regression) among them to the new data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936daf65-2a00-406c-b3c7-178ca370c7ba",
   "metadata": {},
   "source": [
    "# Q2.\n",
    "### How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a187e5bc-3af8-49a7-811a-05abc199ce47",
   "metadata": {},
   "source": [
    "- The optimal value of K is often chosen through techniques like cross-validation or grid search, where different values of K are evaluated on a validation set to select the one that provides the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc43c68-d586-4492-9a56-0c1666eaa024",
   "metadata": {},
   "source": [
    "# Q3.\n",
    "###  What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f1183-49da-472c-b66f-24c835ea5909",
   "metadata": {},
   "source": [
    "- KNN classifier assigns a class label to a new data point based on the majority class among its K nearest neighbors. KNN regressor, on the other hand, predicts a continuous value for the new data point based on the average value among its K nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68447454-d9aa-4d02-a456-2a76356dffdc",
   "metadata": {},
   "source": [
    "# Q4.\n",
    "###  How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c91cab-ddb2-45fc-b1cd-aa3eb099168d",
   "metadata": {},
   "source": [
    "- The performance of KNN can be evaluated using various metrics such as accuracy, precision, recall, F1-score, or RMSE for regression tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cec264-09fc-40f3-a471-13e03efa0a69",
   "metadata": {},
   "source": [
    "# Q5.\n",
    "###  What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a2841-e6cf-4f49-b48b-ba0ee831eb33",
   "metadata": {},
   "source": [
    "- The curse of dimensionality refers to the phenomenon where the performance of KNN deteriorates as the number of dimensions (features) increases. In high-dimensional spaces, the notion of distance becomes less meaningful as data points spread out more uniformly, making it harder to identify nearest neighbors accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bea1524-4f65-4494-b47a-98499bf26891",
   "metadata": {},
   "source": [
    "# Q6.\n",
    "### How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b0e730-e95c-490c-a7d0-533e735f8424",
   "metadata": {},
   "source": [
    "- is to impute them using techniques like mean, median, or mode imputation before applying the algorithm. Another approach is to treat missing values as a separate category or use algorithms that inherently handle missing values, like k-NN imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae580ba-f7df-4b8f-b577-7970ca3e879e",
   "metadata": {},
   "source": [
    "# Q7.\n",
    "### Compare and contrast the performance of the KNN classifier and regressor. Which one is better forwhich type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f0b272-c57b-4d68-bbb0-c64f36762735",
   "metadata": {},
   "source": [
    "- KNN classifier is better suited for classification problems where the decision boundaries are non-linear and the class distribution is not highly imbalanced. KNN regressor, on the other hand, is suitable for regression problems where the relationship between features and target variable is non-linear and there are no significant outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf1b03e-f8e3-4829-8598-8dda8b3b900b",
   "metadata": {},
   "source": [
    "# Q8.\n",
    "### What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8f0aa7-8a32-42f1-b15d-2c0bfbff4ca3",
   "metadata": {},
   "source": [
    "- Strengths : \n",
    "    - Simple and intuitive algorithm.\n",
    "    - Non-parametric, so no assumptions about the underlying data distribution.\n",
    "    - Effective for small to medium-sized datasets.\n",
    "    \n",
    "    \n",
    "- Weaknesses : \n",
    "    - Computationally expensive during prediction, especially with large datasets.\n",
    "    - Sensitivity to the choice of K and the distance metric.\n",
    "    - Performance deteriorates in high-dimensional spaces.\n",
    "    \n",
    "- Addressing weaknesses can involve techniques like dimensionality reduction, algorithm optimization, and careful selection of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082b3428-682d-4189-a4a3-8819fad6a039",
   "metadata": {},
   "source": [
    "# Q9.\n",
    "### What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee72233c-4ec6-411d-8751-c3b23fd31fa4",
   "metadata": {},
   "source": [
    "- Euclidean distance is the straight-line distance between two points in Euclidean space. It is calculated as the square root of the sum of the squared differences between corresponding coordinates. Manhattan distance, also known as L1 distance, is the sum of the absolute differences between corresponding coordinates. While Euclidean distance is sensitive to the overall magnitude of differences, Manhattan distance is less affected by outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad257a2-3d33-4227-b612-50383a692678",
   "metadata": {},
   "source": [
    "# Q10.\n",
    "### What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6436edc8-876f-4377-ae7c-6c17a3b31607",
   "metadata": {},
   "source": [
    "- Feature scaling is important in KNN because the algorithm relies on distance measures to identify nearest neighbors. Features with larger scales may dominate the distance calculation, leading to biased results. Therefore, scaling features to a similar range helps to ensure that all features contribute equally to the distance calculation and improves the performance of KNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
